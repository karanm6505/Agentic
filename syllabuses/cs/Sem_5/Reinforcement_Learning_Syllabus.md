**
# Course Syllabus
## Course Title: Reinforcement Learning
**Course Code:** CS475
**Credits:** 3
**Semester:** Fall 2024
**Instructor:** [Instructor Name Placeholder]
**Contact:** [instructor.email@university.edu]
**Office Hours:** [Office Hours or By Appointment]

### Course Description:
This course provides an introduction to the field of Reinforcement Learning (RL). RL is a subfield of machine learning where an agent learns to make decisions in an environment to maximize some notion of cumulative reward. The course will cover fundamental RL concepts, algorithms, and applications. Topics include Markov Decision Processes, dynamic programming, Monte Carlo methods, temporal difference learning, function approximation, policy gradient methods, and exploration strategies. Students will implement various RL algorithms and apply them to solve challenging control problems.

### Prerequisites:
Linear Algebra, Probability, Introductory Machine Learning, Programming proficiency in Python

### Learning Objectives:
1.  Explain the core concepts of Reinforcement Learning, including agents, environments, rewards, and policies.
2.  Formulate real-world problems as Markov Decision Processes (MDPs).
3.  Implement and compare different RL algorithms, such as dynamic programming, Monte Carlo methods, and temporal difference learning.
4.  Apply function approximation techniques to handle large state spaces.
5.  Design and implement policy gradient methods for continuous control tasks.
6.  Evaluate the performance of RL algorithms using appropriate metrics.

### Required Textbooks:
- Sutton, R. S., & Barto, A. G. *Reinforcement Learning: An Introduction (2nd Edition)*, MIT Press, 2018. (ISBN: 978-0262039246)

### Recommended Readings:
- Article: Silver, D., Huang, A., Maddison, C.J. et al. Mastering the game of Go with deep neural networks and tree search. *Nature* 529, 484–489 (2016).

### Weekly Schedule:
**Week 1: Introduction to Reinforcement Learning**
- Topics: Agents, environments, rewards, policies, Markov Decision Processes (MDPs), RL framework.
- Readings: Sutton & Barto, Chapter 1
- Activities: Introduction lecture, setup Python environment for RL.
- Due: N/A

**Week 2: Markov Decision Processes**
- Topics: Formal definition of MDPs, state space, action space, transition probabilities, reward functions, Markov property.
- Readings: Sutton & Barto, Chapter 3
- Activities: Problem-solving session on MDP formulation.
- Due: N/A

**Week 3: Dynamic Programming**
- Topics: Policy evaluation, policy improvement, policy iteration, value iteration, Bellman equations.
- Readings: Sutton & Barto, Chapter 4
- Activities: Implement policy iteration and value iteration algorithms.
- Due: N/A

**Week 4: Monte Carlo Methods**
- Topics: Monte Carlo prediction, Monte Carlo control, exploring starts, ε-greedy policies.
- Readings: Sutton & Barto, Chapter 5
- Activities: Implement Monte Carlo control for a simple grid world.
- Due: Programming Assignment 1 (MDPs and Dynamic Programming)

**Week 5: Temporal Difference Learning**
- Topics: TD prediction, Sarsa, Q-learning, Expected Sarsa.
- Readings: Sutton & Barto, Chapter 6
- Activities: Implement Sarsa and Q-learning algorithms.
- Due: N/A

**Week 6: n-step Bootstrapping**
- Topics: n-step TD prediction, n-step Sarsa, n-step Tree Backup.
- Readings: Sutton & Barto, Chapter 7
- Activities: Implement n-step Sarsa.
- Due: N/A

**Week 7: Planning and Learning with Tabular Methods**
- Topics: Dyna-Q, Prioritized Sweeping.
- Readings: Sutton & Barto, Chapter 8
- Activities: Implement Dyna-Q algorithm.
- Due: N/A

**Week 8: Midterm Review**
- Topics: Review of topics covered in the first half of the course.
- Readings: Review all previous chapters.
- Activities: Practice problems and Q&A session.
- Due: N/A

**Week 9: Function Approximation**
- Topics: Value function approximation, linear methods, tile coding, radial basis functions, deep neural networks.
- Readings: Sutton & Barto, Chapter 9
- Activities: Implement linear function approximation for value prediction.
- Due: Programming Assignment 2 (Monte Carlo and Temporal Difference Learning)

**Week 10: Policy Gradient Methods**
- Topics: Policy approximation, REINFORCE, actor-critic methods.
- Readings: Sutton & Barto, Chapter 13
- Activities: Implement REINFORCE algorithm.
- Due: N/A

**Week 11: Actor-Critic Methods**
- Topics: Actor-critic with baseline, Advantage Actor-Critic (A2C).
- Readings: Sutton & Barto, Chapter 13
- Activities: Implement A2C algorithm.
- Due: N/A

**Week 12: Exploration and Exploitation**
- Topics: ε-greedy, softmax action selection, upper confidence bound (UCB), Thompson sampling.
- Readings: Sutton & Barto, Chapter 2
- Activities: Comparison of different exploration strategies.
- Due: N/A

**Week 13: Deep Reinforcement Learning**
- Topics: Deep Q-Networks (DQN), Double DQN, Dueling DQN.
- Readings: Research papers on DQN and its variants.
- Activities: Implement DQN algorithm.
- Due: N/A

**Week 14: Advanced Topics in RL**
- Topics: Inverse Reinforcement Learning, Imitation Learning.
- Readings: Selected research papers.
- Activities: Discussion of advanced RL topics.
- Due: Programming Assignment 3 (Function Approximation and Policy Gradients)

**Week 15: Project Presentations**
- Topics: Student project presentations.
- Readings: N/A
- Activities: Project presentations.
- Due: N/A

**Week 16: Final Project Due**
- Topics: N/A
- Readings: N/A
- Activities: N/A
- Due: Final Project

### Assessment Breakdown:
*   Programming Assignments (45%): Three programming assignments focusing on implementing and evaluating different RL algorithms.
*   Final Project (55%): A project involving the application of RL techniques to solve a chosen problem, with a written report and presentation.

### Grading Scale:
A: 90-100%
B: 80-89%
C: 70-79%
D: 60-69%
F: Below 60%