
# Course Syllabus
## Course Title: Natural Language Processing
**Course Code:** CS475
**Credits:** 3
**Semester:** Fall 2024
**Instructor:** [Instructor Name Placeholder]
**Contact:** [instructor.email@university.edu]
**Office Hours:** [Office Hours or By Appointment]

### Course Description:
This course provides an introduction to the fundamental concepts and techniques of Natural Language Processing (NLP). Students will learn how to process, analyze, and understand human language using computational methods. Topics covered include text processing, language modeling, part-of-speech tagging, parsing, semantic analysis, and applications such as machine translation and sentiment analysis. The course emphasizes hands-on experience through programming assignments and a final project.

### Prerequisites:
CS201 (Data Structures), CS301 (Algorithms), or equivalent. Basic Python programming skills are required.

### Learning Objectives:
1.  Implement fundamental NLP techniques such as tokenization, stemming, and lemmatization.
2.  Apply statistical language models to predict the probability of sequences of words.
3.  Utilize part-of-speech tagging and parsing techniques to analyze sentence structure.
4.  Develop machine learning models for text classification and sentiment analysis.
5.  Evaluate the performance of NLP systems using appropriate metrics.
6.  Design and implement an NLP application for a real-world problem.

### Required Textbooks:
- Jurafsky, D., & Martin, J. H. *Speech and Language Processing*. 3rd ed. draft. Retrieved from [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/), 2023.
- Bird, S., Klein, E., & Loper, E. *Natural Language Processing with Python*. O'Reilly Media, 2009. (ISBN: 978-0596516499)

### Recommended Readings:
- Goldberg, Y. *Neural Network Methods for Natural Language Processing*. Morgan & Claypool Publishers, 2017.
- Article: Mikolov, T., Chen, K., Corrado, G., & Dean, J. *Efficient Estimation of Word Representations in Vector Space*. *arXiv preprint arXiv:1301.3781*, 2013.

### Weekly Schedule:
**Week 1: Introduction to Natural Language Processing**
- Topics: Overview of NLP, Applications of NLP, Course logistics
- Readings: Jurafsky & Martin (Chapters 1-2)
- Activities: Introduction to Python and NLTK
- Due: N/A

**Week 2: Text Processing**
- Topics: Tokenization, Stemming, Lemmatization, Regular Expressions
- Readings: Jurafsky & Martin (Chapter 3), Bird et al. (Chapter 3)
- Activities: Lab session on text processing using NLTK
- Due: N/A

**Week 3: Language Modeling**
- Topics: N-gram language models, Smoothing techniques, Evaluation metrics
- Readings: Jurafsky & Martin (Chapter 4)
- Activities: Implement an N-gram language model
- Due: N/A

**Week 4: Text Classification**
- Topics: Naive Bayes, Logistic Regression, Feature engineering
- Readings: Bird et al. (Chapter 6)
- Activities: Lab session on text classification using scikit-learn
- Due: Programming Assignment 1: Text Classification (due end of Week 4)

**Week 5: Part-of-Speech Tagging**
- Topics: Hidden Markov Models, Viterbi algorithm, POS tagging algorithms
- Readings: Jurafsky & Martin (Chapter 5)
- Activities: Implement a POS tagger
- Due: N/A

**Week 6: Parsing**
- Topics: Context-free grammars, Top-down and bottom-up parsing, Dependency parsing
- Readings: Jurafsky & Martin (Chapter 12)
- Activities: Lab session on parsing with NLTK
- Due: N/A

**Week 7: Semantic Analysis**
- Topics: Word Sense Disambiguation, Lexical Semantics, Semantic Role Labeling
- Readings: Jurafsky & Martin (Chapters 17-18)
- Activities: Implement a Word Sense Disambiguation system
- Due: N/A

**Week 8: Midterm Review**
- Topics: Review of topics covered in the first half of the semester
- Readings: Review all previous readings
- Activities: Practice midterm exam
- Due: N/A

**Week 9: Midterm Exam**
- Topics: Comprehensive exam covering all topics from Weeks 1-8
- Readings: N/A
- Activities: Midterm Exam
- Due: Midterm Exam (due end of Week 9)

**Week 10: Word Embeddings**
- Topics: Word2Vec, GloVe, Applications of word embeddings
- Readings: Mikolov et al. (2013)
- Activities: Lab session on training and using word embeddings
- Due: N/A

**Week 11: Recurrent Neural Networks for NLP**
- Topics: RNNs, LSTMs, GRUs
- Readings: Goldberg (Chapter 10)
- Activities: Implement an RNN for text classification
- Due: Programming Assignment 2: Sentiment Analysis with RNNs (due end of Week 11)

**Week 12: Sequence-to-Sequence Models**
- Topics: Encoder-decoder models, Attention mechanisms
- Readings: Goldberg (Chapter 11)
- Activities: Implement a sequence-to-sequence model for machine translation
- Due: N/A

**Week 13: Machine Translation**
- Topics: Statistical machine translation, Neural machine translation
- Readings: Jurafsky & Martin (Chapter 26)
- Activities: Lab session on using pre-trained machine translation models
- Due: N/A

**Week 14: Project Proposals**
- Topics: Discuss project ideas and requirements
- Readings: N/A
- Activities: Project proposal presentations
- Due: Project Proposal (due end of Week 14)

**Week 15: Project Development**
- Topics: Work on individual projects
- Readings: N/A
- Activities: Project development and consultation
- Due: N/A

**Week 16: Project Presentations**
- Topics: Final project presentations
- Readings: N/A
- Activities: Project presentations
- Due: Final Project (due end of Week 16)

### Assessment Breakdown:
*   Programming Assignments: 40% (Two assignments, 20% each)
*   Midterm Exam: 25%
*   Final Project: 35% (Includes proposal, implementation, and presentation)

### Grading Scale:
A: 90-100%
B: 80-89%
C: 70-79%
D: 60-69%
F: Below 60%